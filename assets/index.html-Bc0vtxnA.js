import{_ as r,c as a,b as n,o as t}from"./app-DCpDFG27.js";const o="/imgs/genai/preface/1.genai_stack.png",p={};function s(c,e){return t(),a("div",null,e[0]||(e[0]=[n('<p>自革命性的 ChatGPT 诞生以来，大模型（Large Language Model，LLM）以前所未有的速度飞速发展。各大厂纷纷下场布局，小厂也频有惊艳出品。最典型的是幻方量化出品的 DeepSeek，2024 年 12 月 26 日 DeepSeek-V3 开源发布<sup>[2]</sup>，正式拉开了顶级模型开源的序幕，也向世界证明了中国的创新之力。</p><p>各厂商如同军备竞赛一般，每隔数月，就会有新的模型发布，每次发布必定瞄准榜单第一。在各厂商的“互卷”之下，模型的性能愈发强悍，而使用成本却越来越低，最大受益者成为广泛的 GenAI 用户。</p><p>近年来，GenAI 应用海量涌现<sup>[3]</sup>，以 ChatGPT 为代表的通用 AI 助手能够轻松应对各类内容生成任务；以 Perplexity 为代表的 AI 搜索引擎能够让搜索效率提升数倍；以 Cursor 为代表的 AI 辅助编程应用能够让不懂写代码的人也能开发应用，等等。</p><p>这些革命性的 GenAI 应用的背后不只有 LLM，而是底层硬件到上层应用的一整个 GenAI 技术栈。</p><p><img src="'+o+'" alt="图 1. GenAI Stack"></p><h2 id="硬件层" tabindex="-1"><a class="header-anchor" href="#硬件层"><span>硬件层</span></a></h2><p>硬件是一切的基础，图 1 中列举了与计算强相关的几类主流芯片。</p><p>CPU（Central Processing Unit）是面向通用场景的计算芯片，是冯·诺依曼计算机架构下的核心单元，能够处理复杂负载，可以轻松应对“<em>小量复杂任务</em>”。然而，LLM 的训练和推理主要涉及大量的矩阵运算，属于“<em>大量简单任务</em>”的负载特征，这使得 CPU 在模型训推上显得很吃力。所以，CPU 通常会协同其他 AI 加速芯片一起为训推业务提供算力，CPU 在其中主要用作任务调度或执行一些计算强度较低的算子。随着 CPU 算力的提升，以及 Intel AMX<sup>[4]</sup> 此类面向矩阵运算的高级指令集出现，CPU 也慢慢被用于中小参数模型的推理<sup>[5]</sup>。相比其他 AI 加速芯片，CPU 在 LLM 推理场景具备大内存容量、低起建成本优势，对中小型企业非常友好。</p><p>GPU（Graphics Processing Unit）是如今最火的 AI 加速芯片，最初由英伟达（NVIDIA）推出。NVIDIA GPU 凭借其强悍的计算性能和完善的 CUDA 生态，在 AI 加速芯片市场上占据高达 89% 的份额<sup>[6]</sup>。从 GPU 的全称图形处理单元可看出，它最初被专门用于计算机图像渲染，后因其强大的并行计算能力天然适配 AI 负载的“<em>大量简单任务</em>”的负载特征，被广泛应用于深度学习训练，在大模型时代更是供不应求。与 CPU 相比，GPU 等 AI 加速芯片具备更强的算力和更大的显存（类比 CPU 中的内存）带宽，但显存容量却少了很多，导致大参数规模的模型必须由多卡协同部署，这又带来了新的通信消耗。如何把每张卡充分利用起来成为提升系统性能的关键。</p><p>TPU（Tensor Processing Unit）是 Google 专门为神经网络计算负载设计的一款定制化芯片，对 Tensor 计算，特别是 Google 自家的 TensorFlow 框架进行了深度优化，对 LLM 的训推业务有显著的加速效果。目前，TPU 只在 Google 内部或 Google Cloud 上提供，只此一家。TPU 在并行计算灵活性上不如 GPU，但能效比更高，在大规模包括 LLM 在内的机器学习任务上具有一定的成本优势。</p><blockquote><h4 id="参考" tabindex="-1"><a class="header-anchor" href="#参考"><span>参考</span></a></h4><p>[1] <a href="https://www.bondcap.com/report/tai/" target="_blank" rel="noopener noreferrer">Trends – Artificial Intelligence</a>, Mary Meeker</p><p>[2] <a href="https://api-docs.deepseek.com/zh-cn/news/news1226" target="_blank" rel="noopener noreferrer">DeepSeek-V3 正式发布</a>, DeepSeek</p><p>[3] <a href="https://a16z.com/100-gen-ai-apps-4/" target="_blank" rel="noopener noreferrer">The Top 100 Gen AI Consumer Apps - 4th Edition</a>, a16z</p><p>[4] <a href="https://www.intel.com/content/www/us/en/products/docs/accelerator-engines/advanced-matrix-extensions/overview.html" target="_blank" rel="noopener noreferrer">Intel® Advanced Matrix Extensions (Intel® AMX)</a>, Intel</p><p>[5] <a href="https://www.36kr.com/p/2605400806455937?spm=a2c6h.11547689.0.0.b4983fd2uCdfXR" target="_blank" rel="noopener noreferrer">阿里云弹性计算新升级：CPU上跑推理，模型起建成本降低50%</a>, 36Kr</p><p>[6] <a href="https://www.chyxx.com/cyzx/1213004.html" target="_blank" rel="noopener noreferrer">2025年AI芯片行业市场规模及主要企业市占率分析报告</a>, 智研咨询</p><p>[7] <a href="https://cloud.google.com/tpu/docs/system-architecture-tpu-vm?hl=zh-cn" target="_blank" rel="noopener noreferrer">TPU 架构</a>, Google Cloud</p></blockquote>',11)]))}const l=r(p,[["render",s]]),m=JSON.parse('{"path":"/genai/preface/","title":"软件工程视角下的 GenAI","lang":"zh-CN","frontmatter":{"title":"软件工程视角下的 GenAI","createTime":"2025/06/07 17:31:42","permalink":"/genai/preface/"},"readingTime":{"minutes":3.62,"words":1085},"git":{"createdTime":1749307908000,"updatedTime":1749793309000,"contributors":[{"name":"元闰子","username":"","email":"runxueruan@gmail.com","commits":3,"avatar":"https://gravatar.com/avatar/ed6f4a8bafb5cddb8d8ab06e34b25895424b4f0cf93a7c73bc438cc6adaa00eb?d=retro"}]},"filePathRelative":"notes/genai/前言/genai_at_se_view.md","headers":[]}');export{l as comp,m as data};
